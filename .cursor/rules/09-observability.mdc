---
description: Observability, monitoring, and logging standards for AISTRALE (Phase 2 - Implemented)
globs: backend/**/*.py, frontend/**/*.{ts,tsx}
alwaysApply: true
---

# üìä AISTRALE Observability Standards

**‚ö†Ô∏è CRITICAL**: Phase 2 observability is implemented. All services MUST follow these patterns for logging, metrics, and tracing.

## BuildWorks-09001 Observability Stack (Phase 2 - Implemented)

### Current Implementation
- **Structured Logging**: structlog with JSON output
- **Metrics**: Prometheus with FastAPI instrumentator
- **Tracing**: OpenTelemetry with Jaeger
- **Error Tracking**: Sentry integration
- **Health Checks**: `/health` and `/ready` endpoints

### Service URLs
- **Prometheus**: `http://localhost:9090`
- **Grafana**: `http://localhost:3000` (admin/admin)
- **Jaeger**: `http://localhost:16686`
- **API Metrics**: `http://localhost:16000/metrics`

## BuildWorks-09002 Structured Logging

### Logging Configuration
```python
# ‚úÖ REQUIRED: Structured logging setup (Phase 2 - Implemented)
# backend/core/logging_config.py
import structlog
import logging
import sys

def configure_logging():
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=logging.INFO,
    )

    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.StackInfoRenderer(),
            structlog.dev.set_exc_info,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.JSONRenderer(),
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
```

### Logging Usage
```python
# ‚úÖ GOOD: Structured logging with context
import structlog

logger = structlog.get_logger()

# Info logging
logger.info(
    "inference_completed",
    model="gpt-3.5-turbo",
    tokens=150,
    duration=1.23,
    user_id="user123",
    request_id=request_id
)

# Error logging
try:
    result = await run_inference(model, inputs)
except Exception as e:
    logger.error(
        "inference_failed",
        model=model,
        error=str(e),
        user_id=user_id,
        request_id=request_id,
        exc_info=True
    )
    raise
```

## BuildWorks-09003 Prometheus Metrics

### Metrics Setup
```python
# ‚úÖ REQUIRED: Prometheus metrics (Phase 2 - Implemented)
# backend/main.py
from prometheus_fastapi_instrumentator import Instrumentator

# Instrument Prometheus
Instrumentator().instrument(app).expose(app)
```

### Custom Metrics (Future)
```python
# ‚úÖ GOOD: Custom metrics pattern
from prometheus_client import Counter, Histogram, Gauge

# Inference metrics
INFERENCE_COUNT = Counter(
    'inference_requests_total',
    'Total inference requests',
    ['model', 'provider', 'status']
)

INFERENCE_DURATION = Histogram(
    'inference_duration_seconds',
    'Inference request duration',
    ['model', 'provider']
)

INFERENCE_TOKENS = Histogram(
    'inference_tokens_total',
    'Total tokens used',
    ['model', 'provider']
)

# Usage
INFERENCE_COUNT.labels(
    model=model,
    provider=provider,
    status='success'
).inc()

INFERENCE_DURATION.labels(
    model=model,
    provider=provider
).observe(duration)
```

## BuildWorks-09004 Distributed Tracing

### Tracing Configuration
```python
# ‚úÖ REQUIRED: OpenTelemetry tracing (Phase 2 - Implemented)
# backend/core/tracing.py
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

def configure_tracing(app, service_name="backend"):
    resource = Resource(attributes={
        "service.name": service_name
    })

    trace.set_tracer_provider(TracerProvider(resource=resource))
    tracer_provider = trace.get_tracer_provider()

    otlp_exporter = OTLPSpanExporter(endpoint="http://jaeger:4318/v1/traces")
    span_processor = BatchSpanProcessor(otlp_exporter)
    tracer_provider.add_span_processor(span_processor)

    FastAPIInstrumentor().instrument_app(app)
```

### Tracing Usage
```python
# ‚úÖ GOOD: Manual tracing for LLM calls
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

async def run_inference_with_tracing(model: str, inputs: str):
    """Run inference with tracing."""
    with tracer.start_as_current_span("inference") as span:
        span.set_attribute("model", model)
        span.set_attribute("provider", "huggingface")
        
        try:
            result = await inference_service.run(model, inputs)
            span.set_attribute("tokens", result["tokens"])
            span.set_attribute("status", "success")
            return result
        except Exception as e:
            span.set_attribute("status", "error")
            span.record_exception(e)
            raise
```

## BuildWorks-09005 Error Tracking

### Sentry Integration
```python
# ‚úÖ REQUIRED: Sentry error tracking (Phase 2 - Implemented)
# backend/main.py
import sentry_sdk
from core.config import get_settings

settings = get_settings()

if settings.SENTRY_DSN:
    sentry_sdk.init(
        dsn=settings.SENTRY_DSN,
        traces_sample_rate=1.0,
        profiles_sample_rate=1.0,
    )
```

### Error Handling with Sentry
```python
# ‚úÖ GOOD: Error tracking with context
import sentry_sdk
from core.exceptions import BaseAPIException

try:
    result = await run_inference(model, inputs)
except BaseAPIException as e:
    # Log to Sentry with context
    sentry_sdk.capture_exception(
        e,
        contexts={
            "inference": {
                "model": model,
                "provider": provider,
                "user_id": user_id
            }
        }
    )
    raise
```

## BuildWorks-09006 Health Checks

### Health Check Endpoints
```python
# ‚úÖ REQUIRED: Health check endpoints
from fastapi import APIRouter, HTTPException
from core.database import engine
from sqlmodel import text

router = APIRouter()

@router.get("/health")
async def health_check():
    """Basic health check."""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat()
    }

@router.get("/ready")
async def readiness_check():
    """Readiness check (database connectivity)."""
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        return {"status": "ready"}
    except Exception as e:
        raise HTTPException(
            status_code=503,
            detail=f"Service not ready: {str(e)}"
        )
```

## BuildWorks-09007 Request/Response Logging

### Request Logging Middleware
```python
# ‚úÖ GOOD: Request/response logging middleware
import uuid
import time
import structlog

logger = structlog.get_logger()

@app.middleware("http")
async def logging_middleware(request: Request, call_next):
    """Log all requests and responses."""
    request_id = str(uuid.uuid4())
    request.state.request_id = request_id
    
    start_time = time.time()
    
    # Log request
    logger.info(
        "request_started",
        method=request.method,
        path=request.url.path,
        request_id=request_id,
        client_ip=request.client.host
    )
    
    try:
        response = await call_next(request)
        duration = time.time() - start_time
        
        # Log response
        logger.info(
            "request_completed",
            method=request.method,
            path=request.url.path,
            status_code=response.status_code,
            duration=duration,
            request_id=request_id
        )
        
        response.headers["X-Request-ID"] = request_id
        return response
    except Exception as e:
        duration = time.time() - start_time
        logger.error(
            "request_failed",
            method=request.method,
            path=request.url.path,
            error=str(e),
            duration=duration,
            request_id=request_id,
            exc_info=True
        )
        raise
```

## BuildWorks-09008 Grafana Dashboards

### Dashboard Configuration
- **Location**: `grafana/provisioning/dashboards/`
- **Data Source**: Prometheus
- **Key Metrics**:
  - Request rate
  - Error rate
  - Response time (p50, p95, p99)
  - Inference metrics
  - Token usage

## BuildWorks-09009 Observability Best Practices

### Do's
- ‚úÖ Use structured logging with context
- ‚úÖ Include correlation IDs (request_id)
- ‚úÖ Log at appropriate levels (DEBUG, INFO, WARNING, ERROR)
- ‚úÖ Include relevant context in logs
- ‚úÖ Use metrics for business and technical metrics
- ‚úÖ Trace all external calls (LLM APIs)
- ‚úÖ Monitor error rates and latency

### Don'ts
- ‚ùå Don't log sensitive information (passwords, tokens)
- ‚ùå Don't log at ERROR level for expected errors
- ‚ùå Don't create too many metrics (keep it focused)
- ‚ùå Don't ignore correlation IDs
- ‚ùå Don't log without context

## BuildWorks-09010 LLM-Specific Observability

### LLM Call Tracing
```python
# ‚úÖ GOOD: LLM call observability
async def run_inference_observable(
    model: str,
    inputs: str,
    provider: str
):
    """Run inference with full observability."""
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    logger.info(
        "llm_inference_started",
        model=model,
        provider=provider,
        request_id=request_id,
        input_length=len(inputs)
    )
    
    try:
        with tracer.start_as_current_span("llm_call") as span:
            span.set_attribute("model", model)
            span.set_attribute("provider", provider)
            
            result = await inference_service.run(model, inputs)
            duration = time.time() - start_time
            
            logger.info(
                "llm_inference_completed",
                model=model,
                provider=provider,
                request_id=request_id,
                tokens=result["tokens"],
                duration=duration,
                status="success"
            )
            
            # Update metrics
            INFERENCE_COUNT.labels(
                model=model,
                provider=provider,
                status="success"
            ).inc()
            
            INFERENCE_DURATION.labels(
                model=model,
                provider=provider
            ).observe(duration)
            
            return result
    except Exception as e:
        duration = time.time() - start_time
        logger.error(
            "llm_inference_failed",
            model=model,
            provider=provider,
            request_id=request_id,
            error=str(e),
            duration=duration,
            exc_info=True
        )
        raise
```

---

**Next Steps**: 
- Check metrics: `curl http://localhost:16000/metrics`
- View traces: Open `http://localhost:16686`
- Review dashboards: Open `http://localhost:3000`
- See `11-llm-features.mdc` for LLM-specific observability
